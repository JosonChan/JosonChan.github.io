<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        
        <meta name="keywords" content="Chen Change Loy, Ziwei Liu, Bo Dai, Yixin Cao, Super-resolution, Deep Learning, Computer Vision, Convolutional Neural Network, Enhancement, Inpainting, Knowledge Distillation, Restoration, GAN, Generative Adversarial Networks, Generation, Manipulation, Editing, Object Detection, Segmentation, Recognition, Continual Learning, Domain Generalization, Self-Supervised Learning, Singapore, NTU, Nanyang Technological University, SCSE" />
        
        <!-- Page Title -->
        <title>Publications | VLLab@UESTC</title>

        <!-- Favicons -->
        <link rel="apple-touch-icon" sizes="180x180" href="assets/img/favicons/logo.png">
        <link rel="icon" type="image/png" sizes="32x32" href="assets/img/favicons/logo.png">
        <link rel="icon" type="image/png" sizes="16x16" href="assets/img/favicons/logo.png">
        <link rel="manifest" href="assets/img/favicons/site.webmanifest">
        <link rel="mask-icon" href="assets/img/favicons/safari-pinned-tab.svg" color="#f23838">
        <meta name="msapplication-TileColor" content="#da532c">
        <meta name="theme-color" content="#ffffff">

        <!-- Vendor Stylesheets -->
        <link href="nopathsource/f4dbc32851b099af1b2b4cb6ee659f11" rel="stylesheet">
        <link href="assets/vendor/material-design-iconic-font/dist/css/material-design-iconic-font.min.css" rel="stylesheet">
        <link href="assets/vendor/jquery.mb.vimeo_player/dist/css/jquery.mb.vimeo_player.min.css" rel="stylesheet">
        <link href="assets/vendor/@fortawesome/fontawesome-free/css/all.css" rel="stylesheet">

        <!-- Theme Stylesheets -->
        <link href="assets/css/theme.css" rel="stylesheet">

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="nopathsource/8682f3dbb2db006e1ba53d83b7e3ba44"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'UA-22940424-1');
        </script>
    </head>

    <body>
        <!-- Preloader -->
        <div class="preloader">
            <div class="spinner">
                <div class="circles"></div>
            </div>
        </div>
        <!-- End of Preloader -->

        <!-- Header -->
        <header class="spyre-navbar navbar navbar-expand-lg bg-dark navbar-dark fixed-top align-items-center intro-header" data-transparent data-text-color="#ffffff">
            <div class="container">
                <a class="navbar-brand mr-lg-5 mr-xl-7" href="index.html">
                    <img src="assets/img/logo.png" class="d-none d-lg-block" alt="MMLab" width="100" />
                    <img src="assets/img/logo.png" class="d-block d-lg-none" alt="MMLab" width="150" />
                </a>

                <!-— Desktop Menu -->
                <div class="collapse navbar-collapse" id="navbarSupportedContent">
                    <ul class="navbar-nav mr-auto">
                        <li class="pl-10 nav-item navbar-text"><a href="index.html" class="nav-link text-400">Home</a></li>
                        <li class="pl-5 nav-item navbar-text"><a href="team.html" class="nav-link text-400">Team</a></li>
                        <li class="pl-5 nav-item navbar-text"><a href="research.html" class="nav-link text-400">Research</a></li>
                        <li class="pl-5 nav-item navbar-text"><a href="publication_topic.html" class="nav-link">Publications</a></li>
                        <!-- <li class="pl-5 nav-item navbar-text"><a href="downloads.html" class="nav-link text-400">Code | Datasets</a></li>
                        <li class="pl-5 nav-item navbar-text"><a href="careers.html" class="nav-link text-400">Join Us</a></li> -->
                    </ul>
                </div>
                <!-— End of Desktop Menu -->

                <div class="menu-toggle d-block d-lg-none">
                    <div class="hamburger">
                        <span></span>
                        <span></span>
                        <span></span>
                    </div>
                    <div class="cross">
                        <span></span>
                        <span></span>
                    </div>
                </div>
            </div>

            <!-- Spyrenav Overlay -->
            <div class="spyre-navbar-overlay overlay-slide">
                <div class="container">
                    <div class="row">
                        <div class="spyre-navbar-nav-container col-md-6 col-lg-5 col-xl-4 bg-white ext-l">
                            <nav class="spyre-navbar-nav">
                                <ul class="spyre-nav">
                                    <li class="spyre-nav-item"><a href="index.html" class="spyre-nav-link">Home</a></li>
                                    <li class="spyre-nav-item"><a href="research.html" class="spyre-nav-link">Our Research</a></li>
                                    <li class="spyre-nav-item"><a href="team.html" class="spyre-nav-link">Team</a></li>
                                    <li class="spyre-nav-item dropdown">
                                        <a href="#" class="spyre-nav-link dropdown-toggle" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Publications</a>
                                        <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
                                            <li class="dropdown-menu-item"><a href="publication_topic.html" class="dropdown-menu-link">By Topic</a></li>
                                            <li class="dropdown-menu-item"><a href="publication_year.html" class="dropdown-menu-link">By Year</a></li>
                                        </ul>
                                    </li>
                                    <li class="spyre-nav-item"><a href="downloads.html" class="spyre-nav-link">Code and Datasets</a></li>
                                    <li class="spyre-nav-item"><a href="careers.html" class="spyre-nav-link">Join Us</a></li>
                                </ul>
                            </nav>
                        </div>
        
                        <div class="col-lg-7 col-xl-8 d-none d-md-block">
                            <div class="d-flex flex-column h-100">
                                <div class="d-flex h-100">
                                    <div class="align-self-center">
                                        <div class="text-uppercase"
                                            data-background-text="computer vision"
                                            data-color="#7079a2"
                                            data-opacity="0.02"
                                            data-font-size="85px"
                                            data-font-weight="500"
                                            data-offset-x="-5%"
                                            data-letter-spacing="5px"
                                        ></div>
                                        <div class="text-uppercase"
                                            data-background-text="mmlab"
                                            data-color="#7079a2"
                                            data-opacity="0.04"
                                            data-font-size="175px"
                                            data-font-weight="500"
                                            data-offset-x="29%"
                                            data-padding="7vh 0 2vh 0"
                                            data-letter-spacing="5px"
                                        ></div>
                                        <div class="text-uppercase"
                                            data-background-text="deep learning"
                                            data-color="#7079a2"
                                            data-opacity="0.03"
                                            data-font-size="140px"
                                            data-font-weight="500"
                                            data-offset-x="15%"
                                            data-letter-spacing="5px"
                                        ></div>
                                    </div>
                                </div>
                                
                                <div class="mt-auto">
                                    <ul class="nav flex-nowrap float-right">
                                        <li class="nav-item">
                                            <a class="nav-link px-2" href="https://twitter.com/MMLabNTU" target="_blank">
                                                <i class="zmdi zmdi-twitter text-white"></i>
                                            </a>
                                        </li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <!-- End of Spyrenav Overlay -->
        </header>
        <!-- End of Header -->

        <!-- Main Content -->
        <main class="main minh-100vh">
            <!-- Section -->
            <section class="py-0 overflow-hidden text-center">
                <div class="bg-container parallax" data-rellax-percentage="0.5" style="background-image: url(./assets/img/backgrounds/bg-02.jpg)">
                </div>
            </section>
            <!-- End of Section -->

            <!-- <section class="py-8 py-md-11 overflow-hidden">
                <div class="bg-container parallax" data-rellax-percentage="0.5" style="background-image: url(./assets/img/images/reversion.jpg);"></div>
                <div class="container-fluid">
                    <div class="row align-items-center">
                        <div class="col">
                            <a data-fancybox href="https://www.youtube.com/watch?v=pkal3yjyyKQ" class="icond mx-auto align-items-center text"><i class="zmdi zmdi-play zmdi-hc-4x"></i></a>
                        </div>
                    </div>
                </div>
            </section> -->

            <!-- Section -->
            <section id="section-1" class="pb-0">
                <span id="Generative AI"></span>
                <div class="container">
                    <div class="row">
                        <div class="col-lg-3 order-lg-1">
                            <div class="pb-6 pt-6 py-lg-3" data-toggle="sticky" data-sticky-offset-top="100">
                                <h5 class="mb-4 text-uppercase text-600">Topics</h5>
                                <ul class="mb-5 mb-lg-6 pl-4 text-600">
                                    <li class="mb-1"><a href="#Generative AI" class="text-600">Generative AI</a></li>
                                    <li class="mb-1"><a href="#Cross-modal Understanding" class="text-600">Cross-modal Understanding</a></li>
                                    <li class="mb-1"><a href="#Human-centric Understanding" class="text-600">Human-centric Understanding</a></li>
                                    <li class="mb-1"><a href="#Security AI" class="text-600">Security AI</a></li>
                                    <li class="mb-1"><a href="#Compact Representation" class="text-600">Compact Representation</a></li>
                                    <li class="mb-1"><a href="#Agent and embodied AI" class="text-600">Agent and embodied AI</a></li>
                                </ul>

                                <!-- <h5 class="mb-4 text-uppercase text-600">Sort by</h5>
                                <ul class="mb-5 mb-lg-6 pl-4 text-600">
                                    <li class="mb-1"><a href="publication_year.html#section-1" class="text-600">Years</a></li>
                                </ul> -->

                                <!-- <h5 class="mb-4 text-uppercase text-600">Highlights</h5>
                                <ul class="mb-5 mb-lg-6 pl-4 text-600">
                                    <li class="mb-1"><a href="conference/cvpr2024/index.html" class="text-600">CVPR 2024</a></li>
                                    <li class="mb-1"><a href="conference/neurips2023/index.html" class="text-600">NeurIPS 2023</a></li>
                                    <li class="mb-1"><a href="conference/iccv2023/index.html" class="text-600">ICCV 2023</a></li>
                                    <li class="mb-1"><a href="conference/cvpr2023/index.html" class="text-600">CVPR 2023</a></li>
                                </ul> -->
                            </div>
                        </div>

                        <div class="col-lg-8 order-lg-2 pb-6 pb-lg-0 pt-lg-3">
                            <!-- Generative AI -->
                            <div class="mb-8">
                                <h2 class="mb-4 text-uppercase">Generative AI</h2>
                                <p class="text-700 fw-medium text-justify">To pioneer advanced visual generation technologies that seamlessly blend creativity with artificial intelligence.
                                    Our team is the first to introduce the use of deep neural networks to directly predict super-resolved images. 
                                    We are a group of visual generation researchers who are interested in visual content creation (one of the most important parts in AIGC), 
                                    which can leverage the full power of human creativity in artificial intelligence. 
                                    We aim to study cutting-edge technologies and bring them to the advanced models for the greatest quality show and application. 
                                    Our researches cover <span style="color: blue;">image, video, and 3D generation </span>and editing with a serious of state-of-the-art models published on IEEE TPAMI, ACM MM, AAAI etc.
                                </p>

                                <div class="container">
                                    <div class="row">
                                      <div class="col-4">
                                        <img src="assets/img/publications/vgg/image.jpg" alt="Image 1" class="img-fluid">
                                        <p class="text-center fw-medium">image</p>
                                      </div>
                                      <div class="col-4">
                                        <img src="assets/img/publications/vgg/video.gif" alt="Image 2" class="img-fluid">
                                        <p class="text-center fw-medium">video</p>
                                      </div>
                                      <div class="col-4">
                                        <img src="assets/img/publications/vgg/3d.jpg" alt="Image 3" class="img-fluid">
                                        <p class="text-center fw-medium">3d generation</p>
                                      </div>
                                    </div>
                                </div>
                                <p></p>
                                
                                <h4 class="mb-4 text-uppercase">Image</h4>
                                <div class="row justify-content-center">

                                    <div class="col-lg-4 mb-1 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                        <a href="https://ieeexplore.ieee.org/document/10184483" target="_blank" class="mb-3 d-block position-relative">
                                            <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-dark rounded text-white">
                                                <i class="fas fa-link fa-2x"></i>
                                            </div>
                                            <img src="assets/img/publications/vgg/tmm.jpg" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover" />
                                        </a>
                                    </div>
                                    <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                        <p>
                                            <span class="text-primary">Utilizing Greedy Nature for Multimodal Conditional Image Synthesis in Transformers </span> 
                                                    <br />
                                                    <span class="text-500">
                                                    S. Su, J. Zhu, L. Gao and J. Song <br /> 
                                                    in IEEE Transactions on Multimedia, 2024 <strong>(TMM)</strong><br />
                                                    </span>
                                                    [<a href="https://ieeexplore.ieee.org/document/10184483" target="_blank"><span class="text-muted">PDF</span></a>]
                                                    <!-- [<a href="https://github.com/ZhuGeKongKong/SGG-G2S" target="_blank"><span class="text-muted">Code</span></a>] -->
                                        </p>
                                        <!-- <p class="text-justify">
                                            Making balanced and informative predicate prediction for SGG.
                                        </p> -->
                                    </div>
            
                                    <div class="col-lg-4 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                        <a href="https://users.monash.edu/~yli/assets/pdf/img_syn_tpami2022.pdf" target="_blank" class="mb-3 d-block position-relative">
                                            <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-dark rounded text-white">
                                                <i class="fas fa-link fa-2x"></i>
                                            </div>
                                            <img src="assets/img/teasers/tpami.png" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover" />
                                        </a>
                                    </div>
                                    <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                        <p>
                                            <span class="text-primary">Label-guided generative adversarial network for realistic image synthesis </span> 
                                                    <br />
                                                    <span class="text-500">
                                                    J. Zhu, Lianli Gao, J. Song, Y. Li, F. Zheng, X. Li, and H. T. Shen.<br /> 
                                                    in IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023 <strong>(TPAMI)</strong><br /> 
                                                    </span>
                                                    [<a href="https://users.monash.edu/~yli/assets/pdf/img_syn_tpami2022.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                                    [<a href="https://github.com/RoseRollZhu/Lab2Pix-V2"><span class="text-muted">Code</span></a>]
                                        </p>
                                        <p class="text-justify">
                                            Bridging semantic gap between labels and images for Label-Image Generation.
                                        </p>
                                    </div>
                                </div>

                                <h4 class="mb-4 text-uppercase">Video</h4>
                                    <div class="row justify-content-center">
                                        <div class="col-lg-4 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                            <a href="https://arxiv.org/abs/2403.11535" target="_blank" class="mb-3 d-block position-relative">
                                                <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-dark rounded text-white">
                                                    <i class="fas fa-link fa-2x"></i>
                                                </div>
                                                <img src="assets/img/publications/vgg/echoreel.jpg" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover" />
                                            </a>
                                        </div>
                                        <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                            <p>
                                                <span class="text-primary">EchoReel: Enhancing Action Generation of Existing Video Diffusion Models </span> 
                                                        <br />
                                                        <span class="text-500">
                                                        J. Liu, J Zhu, L, Gao, J. Song<br /> 
                                                        in arXiv, 2024 <strong>(arXiv)</strong><br /> 
                                                        </span>
                                                        [<a href="https://arxiv.org/abs/2403.11535" target="_blank"><span class="text-muted">PDF</span></a>]
                                                        [<a href="https://github.com/liujianzhi/EchoReel"><span class="text-muted">Code</span></a>]
                                                        [<a href="https://liujianzhi.github.io/EchoReel-demo/"><span class="text-muted">Demo</span></a>]
                                            </p>
                                            <!-- <p class="text-justify">
                                                Bridging semantic gap between labels and images for Label-Image Generation.
                                            </p> -->
                                        </div>

                                        <div class="col-lg-4 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                            <a href="https://arxiv.org/abs/2403.11535" target="_blank" class="mb-3 d-block position-relative">
                                                <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-dark rounded text-white">
                                                    <i class="fas fa-link fa-2x"></i>
                                                </div>
                                                <img src="assets/img/publications/vgg/aaai.jpg" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover" />
                                            </a>
                                        </div>
                                        <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                            <p>
                                                <span class="text-primary">F3-Pruning: A Training-Free and Generalized Pruning Strategy towards Faster and Finer Text-to-Video Synthesis </span> 
                                                        <br />
                                                        <span class="text-500">
                                                        S. Su, J. Liu, L, Gao, J. Song<br /> 
                                                        in Association for the Advancement of Artificial Intelligence, 2024 <strong>(AAAI)</strong><br /> 
                                                        </span>
                                                        [<a href="https://arxiv.org/abs/2403.11535" target="_blank"><span class="text-muted">PDF</span></a>]
                                            </p>
                                            <!-- <p class="text-justify">
                                                Bridging semantic gap between labels and images for Label-Image Generation.
                                            </p> -->
                                        </div>
                                    </div>
                                    <span id="Cross-modal Understanding"></span>
                            </div>

                            <!-- CM Understanding -->
                            <div class="mb-8">
                                <h2 class="mb-4 text-uppercase">Cross-modal Understanding</h2>
                                <p class="mb-6 text-700 fw-medium text-justify">Our team is at the forefront of research in cross-modal understanding, 
                                    maintaining leading academic standards and a distinguished reputation. 
                                    We are dedicated to bridging the domain gap between vision and language through the use of cross-modal alignment strategies. 
                                    Our research interests encompass <span style="color: blue;">scene graph generation, cross-modal retrieval, visual question answering, visual captioning and visual grounding. </span>. 
                                    We have published high-impact papers in renowned journals and conferences such as TPAMI, IJCV, TIP, and NeurIPS. Recently, 
                                    our focus has been on leveraging the understanding power of large language models to create a more integrated space between visual and 
                                    linguistic information.
                                </p>

                                <p></p>
                                
                                <h4 class="mb-4 text-uppercase">Scene Gragh Generation</h4>
                                <div class="row justify-content-center">

                                    <div class="col-lg-4 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                        <a href="https://omniobject3d.github.io/" target="_blank" class="mb-3 d-block position-relative">
                                            <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-dark rounded text-white">
                                                <i class="fas fa-link fa-2x"></i>
                                            </div>
                                            <img src="assets/img/teasers/ijcv.png" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover" />
                                        </a>
                                    </div>
                                    <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                        <p>
                                            <span class="text-primary">Informative scene graph generation via debiasing </span> 
                                                    <br />
                                                    <span class="text-500">
                                                    Lianli Gao, X. Lyu, Y. Guo, Y. Hu, Y.-F. Li, L. Xu, H. T. Shen, and J. Song <br /> 
                                                    in International Journal of Computer Vision, 2024 <strong>(IJCV)</strong><br />
                                                    </span>
                                                    [<a href="https://arxiv.org/pdf/2308.05286" target="_blank"><span class="text-muted">PDF</span></a>]
                                                    [<a href="https://github.com/ZhuGeKongKong/SGG-G2S" target="_blank"><span class="text-muted">Code</span></a>]
                                        </p>
                                        <p class="text-justify">
                                            Making balanced and informative predicate prediction for SGG.
                                        </p>
                                    </div>

                                    <div class="col-lg-4 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                        <a href="https://ieeexplore.ieee.org/abstract/document/10192357" target="_blank" class="mb-3 d-block position-relative">
                                            <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-dark rounded text-white">
                                                <i class="fas fa-link fa-2x"></i>
                                            </div>
                                            <img src="assets/img/publications/cmg/tpami.jpg" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover" />
                                        </a>
                                    </div>
                                    <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                        <p>
                                            <span class="text-primary">Adaptive Fine-Grained Predicates Learning for Scene Graph Generation</span> 
                                                    <br />
                                                    <span class="text-500">
                                                    X. Lyu, L. Gao, P. Zeng, H. T. Shen and J. Song.<br /> 
                                                    in IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023 <strong>(TPAMI)</strong><br /> 
                                                    </span>
                                                    [<a href="https://ieeexplore.ieee.org/abstract/document/10192357" target="_blank"><span class="text-muted">PDF</span></a>]
                                                    [<a href="https://github.com/liyuke65535/Part-Aware-Transformer"><span class="text-muted">Code</span></a>]
                                        </p>
                                        <!-- <p class="text-justify">
                                            Bridging semantic gap between labels and images for Label-Image Generation.
                                        </p> -->
                                    </div>

                                    <div class="col-lg-4 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                        <a href="https://arxiv.org/abs/2303.07096" target="_blank" class="mb-3 d-block position-relative">
                                            <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-dark rounded text-white">
                                                <i class="fas fa-link fa-2x"></i>
                                            </div>
                                            <img src="assets/img/publications/cmg/cvpr.jpg" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover" />
                                        </a>
                                    </div>
                                    <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                        <p>
                                            <span class="text-primary">Prototype-based Embedding Network for Scene Graph Generation</span> 
                                                    <br />
                                                    <span class="text-500">
                                                    Chaofan Zheng, Xinyu Lyu, Lianli Gao, Bo Dai, Jingkuan Song.<br />
                                                    in IEEE Conference on Computer Vision and Pattern Recognition, 2023 <strong>(CVPR)</strong><br /> 
                                                    </span>
                                                    [<a href="https://arxiv.org/abs/2303.07096" target="_blank"><span class="text-muted">PDF</span></a>]
                                                    [<a href="https://github.com/VL-Group/PENET"><span class="text-muted">Code</span></a>]
                                        </p>
                                        <!-- <p class="text-justify">
                                            Bridging semantic gap between labels and images for Label-Image Generation.
                                        </p> -->
                                    </div>
                                </div>

                                <h4 class="mb-4 text-uppercase">Cross Model Retrieval</h4>
                                    <div class="row justify-content-center">
                                        <div class="col-lg-4 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                            <a href="https://arxiv.org/abs/2309.17093" target="_blank" class="mb-3 d-block position-relative">
                                                <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-dark rounded text-white">
                                                    <i class="fas fa-link fa-2x"></i>
                                                </div>
                                                <img src="assets/img/publications/cmg/pau.png" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover"/>
                                            </a>
                                        </div>
                                        <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                            <p>
                                                <span class="text-primary">Prototype-based Aleatoric Uncertainty Quantification for Cross-modal Retrieval</span> 
                                                        <br />
                                                        <span class="text-500">
                                                        Hao Li, Jingkuan Song, Lianli Gao, Xiaosu Zhu, Heng Tao Shen<br /> 
                                                        in Advances in Neural Information Processing Systems, 2023<strong>(NeurIPS)</strong><br />
                                                        </span>
                                                        [<a href="https://arxiv.org/abs/2309.17093" target="_blank"><span class="text-muted">PDF</span></a>]
                                                        [<a href="https://github.com/leolee99/PAU"><span class="text-muted">Code</span></a>]
                                                        
                                            </p>
                                            <!-- <p class="text-justify">
                                                Bridging semantic gap between labels and images for Label-Image Generation.
                                            </p> -->
                                        </div>
                                    </div>

                                    <div class="row justify-content-center">
                                        <div class="col-lg-4 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                            <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/4e786a87e7ae249de2b1aeaf5d8fde82-Paper-Conference.pdf" target="_blank" class="mb-3 d-block position-relative">
                                                <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-dark rounded text-white">
                                                    <i class="fas fa-link fa-2x"></i>
                                                </div>
                                                <img src="assets/img/publications/cmg/nips_1.png" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover" />
                                            </a>
                                        </div>
                                        <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                            <p>
                                                <span class="text-primary">A Differentiable Semantic Metric Approximation in Probabilistic Embedding for Cross-Modal Retrieval</span> 
                                                        <br />
                                                        <span class="text-500">
                                                        Hao Li, Jingkuan Song, Lianli Gao, Pengpeng Zeng, Haonan Zhang, Gongfu Li<br /> 
                                                        in Advances in Neural Information Processing Systems, 2022 <strong>(NeurIPS)</strong><br /> 
                                                        </span>
                                                        [<a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/4e786a87e7ae249de2b1aeaf5d8fde82-Paper-Conference.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                                        [<a href="https://github.com/leolee99/2022-NeurIPS-DAA"><span class="text-muted">Code</span></a>]
                                                        <!-- [<a href="https://liujianzhi.github.io/EchoReel-demo/"><span class="text-muted">Demo</span></a>] -->
                                            </p>
                                            <!-- <p class="text-justify">
                                                Bridging semantic gap between labels and images for Label-Image Generation.
                                            </p> -->
                                        </div>
                                    </div>

                                <h4 class="mb-4 text-uppercase">Video Caption</h4>
                                    <div class="row justify-content-center">
                                        <div class="col-lg-4 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                            <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8620348" target="_blank" class="mb-3 d-block position-relative">
                                                <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-dark rounded text-white">
                                                    <i class="fas fa-link fa-2x"></i>
                                                </div>
                                                <img src="assets/img/publications/cmg/tpami_2.jpg" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover" />
                                            </a>
                                        </div>
                                        <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                            <p>
                                                <span class="text-primary">Hierarchical LSTMs with Adaptive Attention for Visual Captioning</span> 
                                                        <br />
                                                        <span class="text-500">
                                                        Gao, Lianli and Li, Xiangpeng and Song, Jingkuan and Shen, Heng Tao<br /> 
                                                        in IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020 <strong>(TPAMI)</strong><br /> 
                                                        </span>
                                                        [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8620348" target="_blank"><span class="text-muted">PDF</span></a>]
                                                        [<a href="https://github.com/lixiangpengcs/Spatial-Temporal-Adaptive-Attention-for-Video-Captioning"><span class="text-muted">Code</span></a>]
                                                        <!-- [<a href="https://liujianzhi.github.io/EchoReel-demo/"><span class="text-muted">Demo</span></a>] -->
                                            </p>
                                            <!-- <p class="text-justify">
                                                Bridging semantic gap between labels and images for Label-Image Generation.
                                            </p> -->
                                        </div>
                                    </div>
                                <span id="Human-centric Understanding"></span>
                            </div>

                            <!-- Human-centric Understanding -->
                            <div class="mb-8">
                                <h2 class="mb-4 text-uppercase">Human-centric Understanding</h2>
                                <p class="mb-6 text-700 fw-medium text-justify">Our team is at the forefront of research in dense pose estimation and generalized person
                                    re-identification, holding leading academic standards and reputation. We are dedicated to advancing
                                    human-centric understanding through the use of deep neural networks. Our members serve as reviewers
                                    for top-tier conferences and journals, including CVPR, TPAMI, IJCV, ICCV, and ECCV.
                                    Our research interests encompass <span style="color: blue;">person re-identification, human parsing and pose estimation</span>,
                                    and we have published numerous papers in prestigious journals and conferences such as CVPR, ICCV, TIP, and ACM MM. 
                                    Recently, our focus has shifted towards constructing large models centered on human understanding and 
                                    leveraging multimodal large models for person retrieval.
                                </p>

                                <p></p>
                                
                                <h4 class="mb-4 text-uppercase">Person Reid</h4>
                                <div class="row justify-content-center">

                                    <div class="col-lg-4 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                        <a href="https://ykdai.github.io/projects/BracketFlare" target="_blank" class="mb-3 d-block position-relative">
                                            <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-dark rounded text-white">
                                                <i class="fas fa-link fa-2x"></i>
                                            </div>
                                            <img src="assets/img/publications/hag/pat.png" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover" />
                                        </a>
                                    </div>
                                    <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                        <p>
                                            <span class="text-primary">Part-aware transformer for generalizable person re-identification</span> 
                                                    <br />
                                                    <span class="text-500">
                                                    Ni, Hao and Li, Yuke and Gao, Lianli and Shen, Heng Tao and Song, Jingkuan.<br /> 
                                                    in IEEE International Conference on Computer Vision, 2023 <strong>(ICCV)</strong><br /> 
                                                    </span>
                                                    [<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ni_Part-Aware_Transformer_for_Generalizable_Person_Re-identification_ICCV_2023_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                                    [<a href="https://github.com/liyuke65535/Part-Aware-Transformer"><span class="text-muted">Code</span></a>]
                                        </p>
                                        <!-- <p class="text-justify">
                                            Bridging semantic gap between labels and images for Label-Image Generation.
                                        </p> -->
                                    </div>

                                    <div class="col-lg-4 mb-1 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                        <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ni_Meta_Distribution_Alignment_for_Generalizable_Person_Re-Identification_CVPR_2022_paper.pdf" target="_blank" class="mb-3 d-block position-relative">
                                            <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-dark rounded text-white">
                                                <i class="fas fa-link fa-2x"></i>
                                            </div>
                                            <img src="assets/img/publications/hag/mda.png" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover" />
                                        </a>
                                    </div>
                                    <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                        <p>
                                            <span class="text-primary">Meta distribution alignment for generalizable person re-identification</span> 
                                                    <br />
                                                    <span class="text-500">
                                                    Ni, Hao and Song, Jingkuan and Luo, Xiaopeng and Zheng, Feng and Li, Wen and Shen, Heng Tao <br /> 
                                                    in IEEE Conference on Computer Vision and Pattern Recognition, 2022 <strong>(CVPR)</strong><br />
                                                    </span>
                                                    [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ni_Meta_Distribution_Alignment_for_Generalizable_Person_Re-Identification_CVPR_2022_paper.pdf" target="_blank"><span class="text-muted">PDF</span></a>]
                                                    [<a href="https://github.com/haoni0812/MDA.git" target="_blank"><span class="text-muted">Code</span></a>]
                                        </p>
                                        <!-- <p class="text-justify">
                                            Making balanced and informative predicate prediction for SGG.
                                        </p> -->
                                    </div>
                                </div>

                                <h4 class="mb-4 text-uppercase">pose estimation</h4>
                                    <div class="row justify-content-center">
                                        <div class="col-lg-4 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                            <a href="https://totoro97.github.io/projects/f2-nerf/" target="_blank" class="mb-3 d-block position-relative">
                                                <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-dark rounded text-white">
                                                    <i class="fas fa-link fa-2x"></i>
                                                </div>
                                                <img src="assets/img/publications/hag/stip.png" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover"/>
                                            </a>
                                        </div>
                                        <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                            <p>
                                                <span class="text-primary">Semantic-aware Transfer with Instance-adaptive Parsing for Crowded Scenes Pose Estimation</span> 
                                                        <br />
                                                        <span class="text-500">
                                                        Wang, Xuanhan and Gao, Lianli and Dai, Yan and Zhou, Yixuan and Song, Jingkuan<br /> 
                                                        in ACM International Conference on Multimedia, 2021 <strong>(ACM-MM)</strong><br /> 
                                                        </span>
                                                        [<a href="https://dl.acm.org/doi/10.1145/3474085.3475233" target="_blank"><span class="text-muted">PDF</span></a>]
                                                        [<a href="https://github.com/stoa-xh91/STIP"><span class="text-muted">Code</span></a>]
                                                        
                                            </p>
                                            <!-- <p class="text-justify">
                                                Bridging semantic gap between labels and images for Label-Image Generation.
                                            </p> -->
                                        </div>

                                        <div class="col-lg-4 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                            <a href="https://ojs.aaai.org/index.php/AAAI/article/download/16206/16013" target="_blank" class="mb-3 d-block position-relative">
                                                <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-dark rounded text-white">
                                                    <i class="fas fa-link fa-2x"></i>
                                                </div>
                                                <img src="assets/img/publications/hag/framework_RSGNet.png" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover" />
                                            </a>
                                        </div>
                                        <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                            <p>
                                                <span class="text-primary">RSGNet: Relation based skeleton graph network for crowded scenes pose estimation</span> 
                                                        <br />
                                                        <span class="text-500">
                                                        Yan Dai, Xuanhan Wang, Lianli Gao, Jingkuan Song, Heng Tao Shen<br /> 
                                                        in Proceedings of the AAAI Conference on Artificial Intelligence, 2021 <strong>(AAAI)</strong><br/>
                                                        </span>
                                                        [<a href="https://ojs.aaai.org/index.php/AAAI/article/download/16206/16013" target="_blank"><span class="text-muted">PDF</span></a>]
                                                        [<a href="https://github.com/vikki-dai/RSGNet"><span class="text-muted">Code</span></a>]
                                                        <!-- [<a href="https://liujianzhi.github.io/EchoReel-demo/"><span class="text-muted">Demo</span></a>] -->
                                            </p>
                                            <!-- <p class="text-justify">
                                                Bridging semantic gap between labels and images for Label-Image Generation.
                                            </p> -->
                                        </div>
                                    </div>

                                <h4 class="mb-4 text-uppercase">Human Parsing</h4>
                                    <div class="row justify-content-center">
                                        <div class="col-lg-4 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                            <a href="https://arxiv.org/abs/2206.10146" target="_blank" class="mb-3 d-block position-relative">
                                                <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-dark rounded text-white">
                                                    <i class="fas fa-link fa-2x"></i>
                                                </div>
                                                <img src="assets/img/publications/hag/ke-rcnn.jpg" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover" />
                                            </a>
                                        </div>
                                        <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                            <p>
                                                <span class="text-primary">KE-RCNN: Unifying Knowledge based Reasoning into Part-level Attribute Parsing</span> 
                                                        <br />
                                                        <span class="text-500">
                                                        Wang, Xuanhan and Song, Jingkuan and Chen, Xiaojia and Cheng, Lechao and Gao, Lianli and Shen, Heng Tao<br /> 
                                                        in IEEE Transactions on Cybernetics, 2022 <strong>(TCYB)</strong><br /> 
                                                        </span>
                                                        [<a href="https://arxiv.org/abs/2206.10146" target="_blank"><span class="text-muted">PDF</span></a>]
                                                        [<a href="https://github.com/JosonChan1998/KE-RCNN"><span class="text-muted">Code</span></a>]
                                                        <!-- [<a href="https://liujianzhi.github.io/EchoReel-demo/"><span class="text-muted">Demo</span></a>] -->
                                            </p>
                                            <!-- <p class="text-justify">
                                                Bridging semantic gap between labels and images for Label-Image Generation.
                                            </p> -->
                                        </div>
                                    </div>
                                <span id="Security AI"></span>
                            </div>
                            
                            <!-- Security AI -->
                            <div class="mb-8">
                                <h2 class="mb-4 text-uppercase">Security AI</h2>
                                <p class="mb-6 text-700 fw-medium text-justify">
                                    Artificial intelligence systems are now widely integrated into daily activities and business environments, frequently aiding in human decision-making. 
                                    Due to their inherent black-box properties, these systems present significant security risks. 
                                    Our team focuses on understanding and controlling these systems to enhance their reliability, trustworthiness, and controllability. 
                                    Our research areas include <span style="color: blue;">adversarial examples, adversarial robustness and machine hallucinations</span>, where we identify vulnerabilities in existing methods, develop robust evaluation techniques, and improve output reliability. 
                                    Our work has been published in top conferences including CVPR, ECCV, NeurIPS, ICLR, etc.
                                </p>

                                <p></p>
                                
                                <h4 class="mb-4 text-uppercase">Adversarial Examples</h4>
                                <div class="row justify-content-center">

                                    <div class="col-lg-4 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                        <a href="https://arxiv.org/pdf/2207.05382" target="_blank" class="mb-3 d-block position-relative">
                                            <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-dark rounded text-white">
                                                <i class="fas fa-link fa-2x"></i>
                                            </div>
                                            <img src="assets/img/publications/aadg/ssa.jpg" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover" />
                                        </a>
                                    </div>
                                    <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                        <p>
                                            <span class="text-primary">Frequency domain model augmentation for adversarial attack</span> 
                                                    <br />
                                                    <span class="text-500">
                                                    Long Y, Zhang Q, Zeng B, et al.<br /> 
                                                    in IEEE European Conference on Computer Vision, 2022 <strong>(ECCV)</strong><br />
                                                    </span>
                                                    [<a href="https://arxiv.org/pdf/2207.05382" target="_blank"><span class="text-muted">PDF</span></a>]
                                                    [<a href="https://github.com/yuyang-long/SSA"><span class="text-muted">Code</span></a>]
                                        </p>
                                    </div>

                                    <div class="col-lg-4 mb-1 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                        <a href="https://arxiv.org/abs/2201.11528" target="_blank" class="mb-3 d-block position-relative">
                                            <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-dark rounded text-white">
                                                <i class="fas fa-link fa-2x"></i>
                                            </div>
                                            <img src="assets/img/publications/aadg/BIA.png" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover" />
                                        </a>
                                    </div>
                                    <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                        <p>
                                            <span class="text-primary">Beyond ImageNet Attack: Towards Crafting Adversarial Examples for Black-box Domains</span> 
                                                    <br />
                                                    <span class="text-500">
                                                    Qilong Zhang, Xiaodan Li, Yuefeng Chen, Jingkuan Song, Lianli Gao, Yuan He, Hui Xue <br /> 
                                                    in International Conference on Learning Representations, 2022 <strong>(ICLR)</strong><br />
                                                    </span>
                                                    [<a href="https://arxiv.org/abs/2201.11528" target="_blank"><span class="text-muted">PDF</span></a>]
                                                    [<a href="https://github.com/qilong-zhang/Beyond-ImageNet-Attack" target="_blank"><span class="text-muted">Code</span></a>]
                                        </p>
                                        <!-- <p class="text-justify">
                                            Making balanced and informative predicate prediction for SGG.
                                        </p> -->
                                    </div>

                                    <div class="col-lg-4 mb-1 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                        <a href="https://arxiv.org/abs/2210.02041" target="_blank" class="mb-3 d-block position-relative">
                                            <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-dark rounded text-white">
                                                <i class="fas fa-link fa-2x"></i>
                                            </div>
                                            <img src="assets/img/publications/aadg/attack_pile.jpg" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover" />
                                        </a>
                                    </div>
                                    <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                        <p>
                                            <span class="text-primary">Natural Color Fool: Towards Boosting Black-box Unrestricted Attacks</span> 
                                                    <br />
                                                    <span class="text-500">
                                                    Shengming Yuan, Qilong Zhang, Lianli Gao, Yaya Cheng, Jingkuan Song <br /> 
                                                    in Advances in Neural Information Processing Systems, 2022 <strong>(NeurIPS)</strong><br />
                                                    </span>
                                                    [<a href="https://arxiv.org/abs/2210.02041" target="_blank"><span class="text-muted">PDF</span></a>]
                                                    [<a href="https://github.com/VL-Group/Natural-Color-Fool" target="_blank"><span class="text-muted">Code</span></a>]
                                        </p>
                                        <!-- <p class="text-justify">
                                            Making balanced and informative predicate prediction for SGG.
                                        </p> -->
                                    </div>
                                </div>

                                <h4 class="mb-4 text-uppercase">adversarial robustness </h4>
                                    <div class="row justify-content-center">
                                        <div class="col-lg-4 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                            <a href="https://arxiv.org/abs/2203.05154" target="_blank" class="mb-3 d-block position-relative">
                                                <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-dark rounded text-white">
                                                    <i class="fas fa-link fa-2x"></i>
                                                </div>
                                                <img src="assets/img/publications/aadg/at.jpg" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover" />
                                            </a>
                                        </div>
                                        <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                            <p>
                                                <span class="text-primary">Practical Evaluation of Adversarial Robustness via Adaptive Auto Attack</span> 
                                                        <br />
                                                        <span class="text-500">
                                                        Ye Liu, Yaya Cheng, Lianli Gao, Xianglong Liu, Qilong Zhang, Jingkuan Song<br /> 
                                                        in IEEE Conference on Computer Vision and Pattern Recognition, 2022 <strong>(CVPR)</strong><br /> 
                                                        </span>
                                                        [<a href="https://arxiv.org/abs/2203.05154" target="_blank"><span class="text-muted">PDF</span></a>]
                                                        [<a href="https://github.com/liuye6666/adaptive_auto_attack"><span class="text-muted">Code</span></a>]
                                                        <!-- [<a href="https://liujianzhi.github.io/EchoReel-demo/"><span class="text-muted">Demo</span></a>] -->
                                            </p>
                                            <!-- <p class="text-justify">
                                                Bridging semantic gap between labels and images for Label-Image Generation.
                                            </p> -->
                                        </div>
                                    </div>
                                    <span id="Compact Representation"></span>
                            </div>

                            <!-- Compact Representation -->
                            <div class="mb-8">
                                <h2 class="mb-4 text-uppercase">Compact Representation</h2>
                                <p class="mb-6 text-700 fw-medium text-justify">
                                    Compact representation group talks things about information thoery and discrete optimization.
                                    We study the compression of billion-scale data and billion-scale model, and how to apply them to practical scenarios.
                                    We believe compression is essential and fundamental for AIGC, where we make the core part of multi-modal tokens. 
                                    To imagine the future of AGI, our current researches focus on <span style="color: blue;">unified multi-modal 
                                    compact representation and model compression</span>. 
                                    The resulting cutting-edge algorithms have been published on CVPR, NeurIPS, SIGIR, IEEE TIP, etc.
                                </p>

                                <p></p>
                                
                                <h4 class="mb-4 text-uppercase">Unified multi-modal representations</h4>
                                <div class="row justify-content-center">

                                    <div class="col-lg-4 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                        <a href="https://arxiv.org/abs/2203.10897" target="_blank" class="mb-3 d-block position-relative">
                                            <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-dark rounded text-white">
                                                <i class="fas fa-link fa-2x"></i>
                                            </div>
                                            <img src="assets/img/publications/crg/crg.jpg" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover" />
                                        </a>
                                    </div>
                                    <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                        <p>
                                            <span class="text-primary">Unified Multivariate Gaussian Mixture for Efficient Neural Image Compression</span> 
                                                    <br />
                                                    <span class="text-500">
                                                    Zhu, Xiaosu and Song, Jingkuan and Gao, Lianli and Zheng, Feng and Shen, Heng Tao.<br /> 
                                                    in IEEE Conference on Computer Vision and Pattern Recognition, 2022 <strong>(CVPR)</strong><br /> 
                                                    </span>
                                                    [<a href="https://arxiv.org/abs/2203.10897" target="_blank"><span class="text-muted">PDF</span></a>]
                                                    [<a href="https://github.com/xiaosu-zhu/McQuic"><span class="text-muted">Code</span></a>]
                                        </p>
                                        <!-- <p class="text-justify">
                                            Bridging semantic gap between labels and images for Label-Image Generation.
                                        </p> -->
                                    </div>

                                    <div class="col-lg-4 mb-1 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                        <a href="https://arxiv.org/abs/2210.05899" target="_blank" class="mb-3 d-block position-relative">
                                            <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-dark rounded text-white">
                                                <i class="fas fa-link fa-2x"></i>
                                            </div>
                                            <img src="assets/img/publications/crg/lb_hash.jpg" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover" />
                                        </a>
                                    </div>
                                    <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                                        <p>
                                            <span class="text-primary">A Lower Bound of Hash Codes' Performance</span> 
                                                    <br />
                                                    <span class="text-500">
                                                    Zhu, Xiaosu and Song, Jingkuan and Lei, Yu and Gao, Lianli and Shen, Hengtao <br /> 
                                                    in Advances in Neural Information Processing Systems, 2022 <strong>(NeurIPS)</strong><br />
                                                    </span>
                                                    [<a href="https://arxiv.org/abs/2210.05899" target="_blank"><span class="text-muted">PDF</span></a>]
                                                    [<a href="https://github.com/VL-Group/LBHash" target="_blank"><span class="text-muted">Code</span></a>]
                                        </p>
                                        <!-- <p class="text-justify">
                                            Making balanced and informative predicate prediction for SGG.
                                        </p> -->
                                    </div>
                                </div>
                                <span id="Agent and embodied AI"></span>
                            </div>

                            <!-- Agent -->
                            <div class="mb-8">
                                <h2 class="mb-4 text-uppercase">Agent and embodied AI</h2>
                                <p class="mb-6 text-700 fw-medium text-justify">Since 2024, our group has been dedicated to developing intelligent agents,
                                    with a special focus on robotic agents integrated into real-world environments.
                                    Our goal is to create systems capable of performing general daily tasks, much like humans do.
                                    To achieve this, we are exploring the potential of multimodal language models for <span style="color: blue;">high-level perception,
                                    reasoning, planning, and learning</span>, as well as generative vision models to <span style="color: blue;">simulate potential future scenarios</span>
                                    for better action generation. Stay tuned for our latest research updates!
                                </p>
                                <p></p>
                            </div>


                        </div>
                        
                    </div>
                </div>
            </section>
            <!-- End of Section -->
        </main>
        <!-- End of Main Content -->


        <!-- Footer -->
        <footer class="footer_p text-white" style="background-image: url(./assets/img/footer-bg.jpg)">
            <div class="container d-flex h-100">
                <div class="row flex-grow-1">
                    <div class="col-lg-4 pt-3 ext-l bg-dark text-center text-lg-left">
                        <div class="d-flex flex-column h-100">
                            <div class="pt-5 pt-lg-7 pb-0">
                                <img src="assets/img/logo.png" alt="" width="100" class="mb-4" />
                                <p class="mb-2 mt-0 fs--1"><br />
                                    No.A307 Innovation center, Qingshuihe Campus <br />
                                    University of Electronic Science and Technology of China<br />
                                </p>
        
                                <p class="fs--1">
                                <span class="text-white"><i class="zmdi zmdi-email zmdi-hc-fw mr-1"></i>juana.alian@gmail.com</span><br />
                                <!-- <span class="text-white"><i class="zmdi zmdi-twitter zmdi-hc-fw mr-1"></i><a href="https://twitter.com/MMLabNTU" target="_blank" class="text-white">@MMLabNTU</a></span></p> -->
                            </div>
    
                            <!-- <ul class="mt-4 mt-lg-auto mb-5 mb-lg-0 list-unstyled list-inline">
                                <li class="mr-3 list-inline-item">
                                    <a href="https://twitter.com/MMLabNTU" target="_blank">
                                        <i class="zmdi zmdi-twitter text-white"></i>
                                    </a>
                                </li>
                            </ul> -->
                        </div>
                    </div>

                    <div class="col d-flex flex-column mb-2 mt-3 pl-lg-7">
                        <div class="row pt-5 pt-lg-8 pb-4 pb-lg-6">
                            <div class="col-6 col-lg-7">
                                <h6 class="mb-1 mb-lg-4 text-uppercase">Publications</h6>
                                <ul class="pt-2 mb-5 fw-light list-unstyled">
                                    <li class="my-1"><a href="publication_topic.html" class="text-white">By Topic</a></li>
                                    <!-- <li class="my-1"><a href="publication_year.html" class="text-white">By Year</a></li> -->
                                </ul>
                            </div>
                            <div class="col-6 col-lg-3">
                                <h6 class="mb-1 mb-lg-4 text-uppercase">About</h6>
                                <ul class="pt-2 mb-5 fw-light list-unstyled">
                                    <li class="my-1"><a href="research.html" class="text-white">Our Research</a></li>
                                    <li class="my-1"><a href="team.html" class="text-white">Team</a></li>
                                    <!-- <li class="my-1"><a href="careers.html" class="text-white">Join Us</a></li> -->
                                </ul>
                            </div>
                            <!-- <div class="col-6 col-lg-3">
                                <h6 class="mb-1 mb-lg-4 text-uppercase">Open Source</h6>
                                <ul class="pt-2 mb-5 fw-light list-unstyled">
                                    <li class="my-1"><a href="https://openmmlab.com/" target="_blank" class="text-white">OpenMMLab</a></li>
                                    <li class="my-1"><a href="downloads.html" class="text-white">Code and Datasets</a></li>
                                </ul>
                            </div> -->
                        </div>

                        <div class="mt-auto d-flex justify-content-between">
                            <span class="fs--3 fs-lg--2">&copy; VLLab@UESTC, 2024</span>
                        </div>
                    </div>
                </div>
            </div>
        </footer>
        <!-- End of Footer -->

        <!-- Top Button -->
        <!-- <a id="back-to-top" href="#" class="btn btn-light btn-lg back-to-top" role="button"><i class="fas fa-chevron-up"></i></a> -->
        <!-- End of Top Button -->

        <!-- Core Javascripts -->
        <script src="assets/vendor/jquery/dist/jquery.min.js"></script>
        <script src="assets/vendor/popper.js/dist/umd/popper.min.js"></script>
        <script src="assets/vendor/bootstrap/dist/js/bootstrap.min.js"></script>
        <script src="assets/vendor/typed.js/lib/typed.min.js"></script>

        <!-- Vendor Javascripts -->
        <script src="assets/vendor/rellax/rellax.min.js"></script>
        <script src="assets/vendor/sticky-kit/dist/sticky-kit.min.js"></script>
        <script src="assets/vendor/imagesloaded/imagesloaded.pkgd.min.js"></script>
        <script src="assets/vendor/isotope-layout/dist/isotope.pkgd.min.js"></script>
        <script src="assets/vendor/isotope-packery/packery-mode.pkgd.min.js"></script>
        <script src="assets/vendor/aos/dist/aos.js"></script>

        <!-- Theme Javascripts -->
        <script src="assets/js/theme.js"></script>
        <script src="assets/js/top.js"></script>
    </body>
</html>